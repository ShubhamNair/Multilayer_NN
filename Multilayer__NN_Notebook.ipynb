{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmDtYYhwpcTr",
        "outputId": "1aea356c-970c-4eec-a70b-b7ddc1743d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Cost after iteration 0: 7022.967233\n",
            "Cost after iteration 100: 7016.033435\n",
            "Cost after iteration 200: 7011.269672\n",
            "Cost after iteration 300: 7007.996450\n",
            "Cost after iteration 400: 7005.747074\n",
            "Cost after iteration 500: 7004.201055\n",
            "Cost after iteration 600: 7003.138301\n",
            "Cost after iteration 700: 7002.407647\n",
            "Cost after iteration 800: 7001.905251\n",
            "Cost after iteration 900: 7001.559764\n",
            "Cost after iteration 1000: 7001.322158\n",
            "Cost after iteration 1100: 7001.158731\n",
            "Cost after iteration 1200: 7001.046316\n",
            "Cost after iteration 1300: 7000.968987\n",
            "Cost after iteration 1400: 7000.915789\n",
            "Cost after iteration 1500: 7000.879190\n",
            "Cost after iteration 1600: 7000.854011\n",
            "Cost after iteration 1700: 7000.836687\n",
            "Cost after iteration 1800: 7000.824767\n",
            "Cost after iteration 1900: 7000.816566\n",
            "Cost after iteration 2000: 7000.810923\n",
            "Cost after iteration 2100: 7000.807040\n",
            "Cost after iteration 2200: 7000.804369\n",
            "Cost after iteration 2300: 7000.802530\n",
            "Cost after iteration 2400: 7000.801265\n",
            "MNIST Test Accuracy: 11.35%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "\n",
        "def load_images_from_folder(folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        with Image.open(img_path) as img:\n",
        "            img = img.resize((64, 64)).convert('L')  # Resize and convert to grayscale\n",
        "            images.append(np.asarray(img) / 255.0)  # Normalize pixel values\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return z > 0\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "def initialize_weights(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    weights = {}\n",
        "    L = len(layer_dims)\n",
        "    for l in range(1, L):\n",
        "        weights['W' + str(l)] = np.random.randn(layer_dims[l - 1], layer_dims[l]) * 0.01\n",
        "        weights['b' + str(l)] = np.zeros((1, layer_dims[l]))\n",
        "    return weights\n",
        "\n",
        "\n",
        "def forward_propagation(X, weights):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(weights) // 2\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        W = weights['W' + str(l)]\n",
        "        b = weights['b' + str(l)]\n",
        "        Z = np.dot(A_prev, W) + b\n",
        "        A = relu(Z)\n",
        "        caches.append((A_prev, W, b, Z))\n",
        "    W = weights['W' + str(L)]\n",
        "    b = weights['b' + str(L)]\n",
        "    Z = np.dot(A, W) + b\n",
        "    AL = sigmoid(Z)\n",
        "    caches.append((A, W, b, Z))\n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[0]\n",
        "    cost = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / m\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "\n",
        "def backward_propagation(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[0]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    current_cache = caches[L - 1]\n",
        "    A_prev, W, b, Z = current_cache\n",
        "    dZ = dAL * sigmoid_derivative(Z)\n",
        "    dW = np.dot(A_prev.T, dZ) / m\n",
        "    db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "    dA_prev = np.dot(dZ, W.T)\n",
        "    grads[\"dW\" + str(L)] = dW\n",
        "    grads[\"db\" + str(L)] = db\n",
        "    for l in reversed(range(L - 1)):\n",
        "        current_cache = caches[l]\n",
        "        A_prev, W, b, Z = current_cache\n",
        "        dZ = dA_prev * relu_derivative(Z)\n",
        "        dW = np.dot(A_prev.T, dZ) / m\n",
        "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        if l > 0:\n",
        "            dA_prev = np.dot(dZ, W.T)\n",
        "        grads[\"dW\" + str(l + 1)] = dW\n",
        "        grads[\"db\" + str(l + 1)] = db\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_parameters(weights, grads, learning_rate):\n",
        "    L = len(weights) // 2\n",
        "    for l in range(L):\n",
        "        weights[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        weights[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
        "    return weights\n",
        "\n",
        "\n",
        "def model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=1000, print_cost=False):\n",
        "    np.random.seed(1)\n",
        "    costs = []\n",
        "    weights = initialize_weights(layers_dims)\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = forward_propagation(X, weights)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = backward_propagation(AL, Y, caches)\n",
        "        weights = update_parameters(weights, grads, learning_rate)\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "            costs.append(cost)\n",
        "    return weights\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "(X_mnist, y_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "# Filter out only the images for digits 0 and 1\n",
        "filter_indices_mnist = (y_mnist == 0) | (y_mnist == 1)\n",
        "X_mnist, y_mnist = X_mnist[filter_indices_mnist], y_mnist[filter_indices_mnist]\n",
        "\n",
        "# Normalize the images from 0-255 to 0-1\n",
        "X_mnist = X_mnist / 255.0\n",
        "X_test_mnist = X_test_mnist / 255.0\n",
        "\n",
        "# Flatten the images for the MLP (multilayer perceptron)\n",
        "X_mnist = X_mnist.reshape((-1, 28 * 28))\n",
        "X_test_mnist = X_test_mnist.reshape((-1, 28 * 28))\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train_mnist, X_val_mnist, y_train_mnist, y_val_mnist = train_test_split(X_mnist, y_mnist, test_size=0.2, random_state=32)\n",
        "\n",
        "# Define layer dimensions for the model\n",
        "layer_dims = [X_train_mnist.shape[1], 10, 8, 8, 4, 1]\n",
        "\n",
        "# Train the model\n",
        "trained_weights_mnist = model(X_train_mnist, y_train_mnist, layer_dims, learning_rate=0.0075,\n",
        "                               num_iterations=2500, print_cost=True)\n",
        "\n",
        "# Evaluate on the MNIST test set\n",
        "AL_test_mnist, _ = forward_propagation(X_test_mnist, trained_weights_mnist)\n",
        "predictions_mnist = AL_test_mnist > 0.5\n",
        "\n",
        "accuracy_mnist = accuracy_score(y_test_mnist, predictions_mnist)\n",
        "print(f'MNIST Test Accuracy: {accuracy_mnist * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frSpXfaM5rWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}